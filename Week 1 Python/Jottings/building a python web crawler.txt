building a basic crawler

To get started, use requests to download the first page. Behind the scene, the get() method performs an HTTP GET request to the specified URL.
response = requests.get("https://www.scrapingcourse.com/ecommerce/")

response.content will now contain the HTML document produced by the server. Feed it to BeautifulSoup. The "html.parser" option specifies the parser the library will use.

soup = BeautifulSoup(response.content, "html.parser")

Select all HTML link elements on the page. select() applies a CSS selector strategy, returning all <a> elements with an href attribute. That's how you can identify links in an HTML document.

link_elements = soup.select("a[href]")

Populate a list with the shop URLs extracted from the link elements. Use an if condition to avoid empty and external URLs.
urls = []
for link_element in link_elements:
   url = link_element['href']
   if "https://www.scrapingcourse.com/ecommerce/" in url:
      urls.append(url)

Extend the logic by repeating the same procedure for each new page. The script below keeps crawling the site as long as there are shop pages left to visit. URLs discovered here include pagination and product pages.